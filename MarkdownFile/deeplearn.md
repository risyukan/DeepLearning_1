# 从零开始的深度学习笔记
## 1.multi-layered perceptron
![alt text](1730010147874.png)
看上去是3层构造，但是实际上只有两层有权重，所以是2层感知机。
## 2.sigmoid function
![alt text](78a0a59172e4ea4dd75e711a0d8239a.png)
## 3.sigmoid函数图像
![alt text](1730638215829.png)
線形関数の問題点は、どんなに層を深くしても、それと同じことを行う「隠れ層のないネットワーク」が必ず存在する、という事実に起因します。
## 4.ReLU函数
![alt text](1733569806632.png)
NumPyのmaximumという関数を使っています。このmaximumは、入力された値から大きいほうの値を選んで出力する関数です。
## 5.神经网络
![alt text](1730726168029.png)
这里需要注意的是，numpy里的1维行列可以看作行，也可以看作列，来进行计算。
![alt text](1730875296950.png)
![alt text](1733229448798.png)
**<font color=red>从后神经元往前对应是行列的列，从前神经元往后对应是行列的行</font>**
机器学习中的问题可以大致分类为回归问题和分类问题，回归问题比如说从人物图像中预测这个人的体重等一系列的数值。
**分类问题使用softmax函数**
## 6.softmax函数
![alt text](1730891791059.png)
![alt text](1730891832102.png)
![alt text](1730979309947.png)
ソフトマックスの指数関数の計算を行う際には、何らかの定数を足し算（もしくは、引き算）しても結果は変わらない、ということです。

**对于输入ak（n个数），加上任意常数，不改变结果**

ここでC′にはどのような値を用いることもできますが、オーバーフローの対策としては、入力信号の中で最大の値を用いることが一般的です。

**一般情况下在模型推论时，会省略softmax函数。那为什么要在输出层使用softmax函数呢？ 原因与模型学习时有关。** 

输出层的个数 = 分类问题中类别的个数

## 7.MNISTデータセット
load_mnist 関数は、「(訓練画像, 訓練ラベル), (テスト画像, テストラベル)」という形式で、読み込んだMNISTデータを返します。また、引数として、load_mnist(normalize=True, flatten=True, one_hot_label=False) のように、3つの引数を設定することができます。
**第一个参数可以把画像的像素通道正则化0到1之间**
normalize をTrue に設定すると、その関数の内部では画像の各ピクセルの値を255で除算し、データの値が0.0～1.0の範囲に収まるように変換されます。このようなデータをある決まった範囲に変換する処理を正規化（normalization）と言います。
**第二个参数可以把画像的1x28x28的三维矩阵化为784的一维矩阵**
**第三个参数可以把图像标签转换为one-hot形式，这个形式可以把2转换成【0，0，1，0，0，0，0，0，0，0】**

**<font color=red>加载这个数据集的时候，要把当前工作目录设置在运行脚本这个目录上，然后使用代码把上一级的目录加入到查找路径中，这样才能正确导入dataset这个文件里的mnist.py</font>**

## 8.批处理
![alt text](1733482027151.png)
バッチ処理には、コンピュータで計算する上で大きな利点があります。それは、バッチ処理によって、1枚あたりの処理時間を大幅に短縮できるという利点です。なぜ処理時間を短縮できるかというと、値計算を扱うライブラリの多くは、大きな配列の計算を効率良く処理できるような高度な最適化が行われているからです。また、ニューラルネットワークの計算において、データ転送がボトルネックになる場合は、バッチ処理を行うことで、バス帯域の負荷を軽減することができます **<font color=red>（正確には、データの読み込みに対して演算の割合を多くすることができます）</font>**。つまり、バッチ処理を行うことで大きな配列の計算を行うことになりますが、大きな配列を一度に計算するほうが、分割した小さい配列を少しずつ計算するよりも速く計算が完了するのです。

## 9.从数据中学习
そのひとつの方法としては、画像から特徴量を抽出して、その特徴量のパターンを機械学習の技術で学習する方法が考えられます。ここで言う特徴量とは、入力データ（入力画像）から本質的なデータ（重要なデータ）を的確に抽出できるように設計された変換器を指します。
![alt text](1733640705058.png)

## 10.  2乗和誤差
![alt text](1733656543520.png)
当值偏差大时，损失函数的值也大

## 11.交差エントロピー誤差（cross entropy error）
![alt text](1733745816568.png)
当值偏差大时，损失函数的值也大
![alt text](1733747323181.png)

## 12.ミニバッチ学習
![alt text](1733827804710.png)
n代表数据个数，k代表单个数据的某个值（也等于分类的类别）
除以N之后，相当于是算单个数据的平均损失函数。这样的话损失函数的大小就与训练数据的多少无关了。

**<font color=red>当训练数据过多时，计算总的损失函数会很困难，所以可以分批次计算损失函数，进行模型学习</font>**

在 NumPy 中，当我们用 shape 查看一个数组的形状时，它返回的是一个描述维度的元组。这个元组是对数组的结构描述，而不是数组（矩阵）本身的内容。

模型训练时用小部分数据的损失函数来代替所有数据的损失函数。

## 13.之所以使用损失函数作为指标，而不使用识别精度作为指标的原因
<font color=red>認識精度を指標にしてはいけない理由は、微分がほとんどの場所で0になってしまい、パラメータの更新ができなくなってしまうからなのです。</font>

## 14. 数値微分
丸め誤差：np.float32()会把小数点45位以后的数省略。
需注意数值精度和因为h无法无限趋近于零导致的误差

## 15.勾配法（gradient method）
![alt text](1734172688453.png)
ηは更新の量を表します。学習率（learning rate）

ニューラルネットワークの学習においては、学習率の値を変更しながら、正しく学習できているかどうか、確認作業を行うのが一般的です。

学習率が大きすぎると、大きな値へと発散してしまいます。逆に、学習率が小さすぎると、ほとんど更新されずに終わってしまいます。つまりは、適切な学習率を設定するということが重要な問題になるということです。

学習率のようなパラメータはハイパーパラメータと言います。一般的には、このハイパーパラメータをいろいろな値で試しながら、うまく学習できるケースを探すという作業が必要になります。